{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "from seleniumbase import SB\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import re\n",
    "\n",
    "def extract_date(string):\n",
    "    # Regular expression to match the date format (e.g., 01 Apr 2023)\n",
    "    match = re.search(r'\\d{2} \\w{3} \\d{4}', string)\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "\n",
    "# Configure logging to show timestamps and error details.\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "def scrape_ronin_chain_token_transfers(csv_file_with_axie_id):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_with_axie_id)\n",
    "        axie_ids = df.iloc[:, 0].astype(str).tolist()  # Ensure Axie IDs are strings\n",
    "        all_data = []\n",
    "\n",
    "        with SB(uc=True, headless=False) as sb:\n",
    "            for axie_id in axie_ids:\n",
    "                try:\n",
    "                    url = f\"https://app.roninchain.com/token/0x32950db2a7164ae833121501c797d79e7b79d74c/{axie_id}?p=1&ps=25\"\n",
    "                    logging.info(f\"Processing Axie ID: {axie_id}\")\n",
    "                    sb.open(url)\n",
    "\n",
    "                    WebDriverWait(sb.driver, 30).until(\n",
    "                        EC.presence_of_element_located((By.CLASS_NAME, \"ronin-table-tbody\"))\n",
    "                    )\n",
    "\n",
    "                    sb.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    time.sleep(0)  # Increased sleep\n",
    "\n",
    "                    rows = sb.find_elements(By.CLASS_NAME, \"ronin-table-row\")\n",
    "                    logging.info(f\"Axie ID {axie_id}: Found {len(rows)} transaction rows.\")\n",
    "\n",
    "                    for row in rows:\n",
    "                        try:\n",
    "                            cells = row.find_elements(By.CLASS_NAME, \"ronin-table-cell\")\n",
    "                            if len(cells) >= 1:\n",
    "                                try:\n",
    "                                    anchor = cells[0].find_element(By.TAG_NAME, \"a\")\n",
    "                                    href = anchor.get_attribute(\"href\").strip()\n",
    "                                    tx_hash = href.split(\"/tx/\")[-1] if \"/tx/\" in href else href\n",
    "\n",
    "                                    all_data.append({\n",
    "                                        'Axie ID': axie_id,\n",
    "                                        'Tx Hash': tx_hash\n",
    "                                    })\n",
    "                                except NoSuchElementException:\n",
    "                                    logging.warning(f\"No anchor tag found in row for Axie ID {axie_id}\")\n",
    "                                    continue  # Skip this row\n",
    "                                except Exception as anchor_ex:\n",
    "                                    logging.error(f\"Error processing anchor tag for Axie ID {axie_id}: {anchor_ex}\", exc_info=True)\n",
    "                                    continue\n",
    "                        except Exception as row_ex:\n",
    "                            logging.error(f\"Error extracting row data for Axie ID {axie_id}: {row_ex}\", exc_info=True)\n",
    "                            continue\n",
    "                except Exception as id_ex:\n",
    "                    logging.error(f\"Error processing Axie ID {axie_id}: {id_ex}\", exc_info=True)\n",
    "                    continue\n",
    "\n",
    "        final_df = pd.DataFrame(all_data)\n",
    "        return final_df\n",
    "\n",
    "    except Exception as overall_ex:\n",
    "        logging.error(f\"Scraping failed: {overall_ex}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def extract_dates_from_csv(csv_filename):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filename)\n",
    "        results = []\n",
    "        base_url = \"https://app.roninchain.com/tx/\"\n",
    "\n",
    "        with SB(uc=True, headless=False) as sb:\n",
    "            for index, row in df.iterrows():\n",
    "                try:\n",
    "                    tx_hash = str(row['Tx Hash']).strip()\n",
    "                    full_url = base_url + tx_hash\n",
    "                    print(f\"Processing URL: {full_url}\")\n",
    "                    sb.open(full_url)\n",
    "                    \n",
    "                    WebDriverWait(sb.driver, 30).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"div.-mb-8\"))\n",
    "                    )\n",
    "                    time.sleep(0)\n",
    "                    \n",
    "                    try:\n",
    "                        date_div = sb.find_element(By.CSS_SELECTOR, \"div.-mb-8\")\n",
    "                        date_text = date_div.text.strip()\n",
    "                    except NoSuchElementException as inner_e:\n",
    "                        print(f\"Error finding date element for tx_hash {tx_hash}: {inner_e}\")\n",
    "                        date_text = \"N/A\"\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"General error finding date element for tx_hash {tx_hash}: {e}\", exc_info=True)\n",
    "                        date_text = \"N/A\"\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Tx Hash': tx_hash,\n",
    "                        'Date': extract_date(date_text),  # Ensure extract_date is defined elsewhere\n",
    "                        'Axie_id': str(row['Axie ID']).strip()\n",
    "                    })\n",
    "                except Exception as row_ex:\n",
    "                    logging.error(f\"Error processing row {index} in CSV: {row_ex}\", exc_info=True)\n",
    "                    continue\n",
    "        \n",
    "        result_df = pd.DataFrame(results)\n",
    "        if not result_df.empty:  # Only save if there's data\n",
    "            result_df.to_csv(\"tx_dates.csv\", mode='a', index=False, header=False)\n",
    "            print(\"Data saved to tx_dates.csv\")\n",
    "        else:\n",
    "            print(\"No data to save to tx_dates.csv\")\n",
    "    \n",
    "    except FileNotFoundError as fnf_error:\n",
    "        print(f\"CSV file not found: {fnf_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Operation failed: {e}\")\n",
    "\n",
    "# Usage example\n",
    "for i in range(1, 3):\n",
    "    try:\n",
    "        df = scrape_ronin_chain_token_transfers(f\"axie_ids_{i}.csv\")\n",
    "        if not df.empty:\n",
    "            logging.info(\"Data extracted successfully:\")\n",
    "            print(df)\n",
    "            df.to_csv(f\"ronin_transfers{i}.csv\", index=False)\n",
    "            logging.info(f\"Data saved to ronin_transfers{i}.csv\")\n",
    "        else:\n",
    "            logging.info(\"No data found or extraction encountered errors!\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in scraping iteration {i}: {e}\", exc_info=True)\n",
    "\n",
    "for i in range(1, 3):\n",
    "    try:\n",
    "        # Check if the file exists before attempting to process it\n",
    "        if os.path.exists(f\"ronin_transfers{i}.csv\"):\n",
    "            extract_dates_from_csv(f\"ronin_transfers{i}.csv\")\n",
    "        else:\n",
    "            logging.warning(f\"File ronin_transfers{i}.csv not found, skipping date extraction.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in extracting dates for iteration {i}: {e}\", exc_info=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
